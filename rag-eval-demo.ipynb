{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŽ¯ RAGAS Evaluation Demo with EPAM DIAL\n",
        "\n",
        "This notebook demonstrates how to use **RAGAS** (Retrieval-Augmented Generation Assessment) to evaluate RAG systems using EPAM DIAL models.\n",
        "\n",
        "## ðŸ“‹ What We'll Learn:\n",
        "1. **Setup** - Connect to EPAM DIAL API\n",
        "2. **Dataset** - Load our fake evaluation dataset\n",
        "3. **Models** - Configure LLM and embedding models\n",
        "4. **Evaluation** - Run RAGAS metrics\n",
        "5. **Results** - Analyze and export results\n",
        "\n",
        "## ðŸ”§ RAGAS Metrics We'll Evaluate:\n",
        "- **Context Recall** - How complete is the retrieved context?\n",
        "- **Context Precision** - How relevant is the retrieved context?\n",
        "- **Faithfulness** - Is the answer grounded in the context?\n",
        "- **Answer Accuracy** - How accurate is the answer compared to ground truth?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SUCCESS] All imports successful!\n",
            "[INFO] Libraries loaded:\n",
            "  - pandas for data handling\n",
            "  - datasets for RAGAS format\n",
            "  - utils for EPAM DIAL integration\n",
            "  - ragas for evaluation metrics\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Import Required Libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from datasets import Dataset\n",
        "\n",
        "# Import our custom utilities\n",
        "from utils import (\n",
        "    create_fake_ragas_dataset, \n",
        "    create_ragas_dataset,\n",
        "    create_langchain_llm, \n",
        "    create_langchain_embeddings\n",
        ")\n",
        "\n",
        "# Import RAGAS components\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    context_recall,\n",
        "    context_precision, \n",
        "    faithfulness,\n",
        "    answer_correctness\n",
        ")\n",
        "\n",
        "print(\"[SUCCESS] All imports successful!\")\n",
        "print(\"[INFO] Libraries loaded:\")\n",
        "print(\"  - pandas for data handling\")\n",
        "print(\"  - datasets for RAGAS format\")\n",
        "print(\"  - utils for EPAM DIAL integration\") \n",
        "print(\"  - ragas for evaluation metrics\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š Step 2: Load Evaluation Dataset\n",
        "\n",
        "Let's load our fake dataset that contains the 4 required columns for RAGAS evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Loading Evaluation Dataset...\n",
            "========================================\n",
            "Dataset shape: (5, 5)\n",
            "Columns: ['question', 'answer', 'context', 'ground_truth', 'retrieved_contexts']\n",
            "\n",
            "First few rows:\n",
            "                                  question  \\\n",
            "0           What is the capital of France?   \n",
            "1  What are the main ingredients in pizza?   \n",
            "2            How does photosynthesis work?   \n",
            "3         What is the population of Tokyo?   \n",
            "4              Who wrote Romeo and Juliet?   \n",
            "\n",
            "                                              answer  \\\n",
            "0                    The capital of France is Paris.   \n",
            "1  Pizza typically contains dough, tomato sauce, ...   \n",
            "2  Plants use sunlight, water, and carbon dioxide...   \n",
            "3         Tokyo has approximately 14 million people.   \n",
            "4  William Shakespeare wrote Romeo and Juliet, on...   \n",
            "\n",
            "                                             context  \\\n",
            "0  France is a country located in Western Europe....   \n",
            "1  Pizza is a popular Italian dish consisting of ...   \n",
            "2  Photosynthesis is the process by which plants ...   \n",
            "3  Tokyo is the capital city of Japan and one of ...   \n",
            "4  Romeo and Juliet is a tragic play written by W...   \n",
            "\n",
            "                                        ground_truth  \\\n",
            "0                                              Paris   \n",
            "1          Dough, tomato sauce, cheese, and toppings   \n",
            "2  Plants convert sunlight, water, and CO2 into g...   \n",
            "3                    37 million (Greater Tokyo Area)   \n",
            "4                                William Shakespeare   \n",
            "\n",
            "                                  retrieved_contexts  \n",
            "0  [France is a country located in Western Europe...  \n",
            "1  [Pizza is a popular Italian dish consisting of...  \n",
            "2  [Photosynthesis is the process by which plants...  \n",
            "3  [Tokyo is the capital city of Japan and one of...  \n",
            "4  [Romeo and Juliet is a tragic play written by ...  \n",
            "\n",
            "[INFO] Dataset Summary:\n",
            "  - Total examples: 5\n",
            "  - Questions: 5 unique\n",
            "  - Average answer length: 71.4 characters\n",
            "  - Average context length: 254.6 characters\n",
            "\n",
            "[SUCCESS] Dataset ready for RAGAS evaluation!\n"
          ]
        }
      ],
      "source": [
        "# Load our fake dataset\n",
        "print(\"[INFO] Loading Evaluation Dataset...\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Create the dataset directly as RAGAS-compatible format\n",
        "ragas_dataset = create_ragas_dataset()\n",
        "\n",
        "print(f\"Dataset shape: {ragas_dataset.shape}\")\n",
        "print(f\"Columns: {ragas_dataset.column_names}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(ragas_dataset.to_pandas().head())\n",
        "\n",
        "print(\"\\n[INFO] Dataset Summary:\")\n",
        "print(f\"  - Total examples: {len(ragas_dataset)}\")\n",
        "print(f\"  - Questions: {len(set(ragas_dataset['question']))} unique\")\n",
        "print(f\"  - Average answer length: {sum(len(a) for a in ragas_dataset['answer']) / len(ragas_dataset):.1f} characters\")\n",
        "print(f\"  - Average context length: {sum(len(c) for c in ragas_dataset['context']) / len(ragas_dataset):.1f} characters\")\n",
        "\n",
        "print(f\"\\n[SUCCESS] Dataset ready for RAGAS evaluation!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ¤– Step 3: Configure LangChain Models for RAGAS\n",
        "\n",
        "Now let's create our LangChain wrappers for the EPAM DIAL models that RAGAS will use for evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Configuring LangChain Models for RAGAS...\n",
            "==================================================\n",
            "[SUCCESS] LangChain LLM configured: gpt-4.1-mini-2025-04-14\n",
            "[SUCCESS] LangChain Embeddings configured: text-embedding-3-small-1\n",
            "\n",
            "[INFO] Model Configuration Complete!\n",
            "These LangChain wrappers will be used for:\n",
            "  - LangChain LLM: Faithfulness & Answer Accuracy evaluation\n",
            "  - LangChain Embeddings: Context Recall & Precision evaluation\n"
          ]
        }
      ],
      "source": [
        "# Configure our EPAM DIAL models for RAGAS\n",
        "print(\"[INFO] Configuring LangChain Models for RAGAS...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create LangChain LLM wrapper (for Faithfulness & Answer Accuracy)\n",
        "# Using original deployments that have access\n",
        "langchain_llm = create_langchain_llm(deployment_name=\"gpt-4.1-mini-2025-04-14\")\n",
        "print(f\"[SUCCESS] LangChain LLM configured: gpt-4.1-mini-2025-04-14\")\n",
        "\n",
        "# Create LangChain Embedding wrapper (for Context Recall & Precision)\n",
        "# Using original deployments that have access\n",
        "langchain_embeddings = create_langchain_embeddings(deployment_name=\"text-embedding-3-small-1\")\n",
        "print(f\"[SUCCESS] LangChain Embeddings configured: text-embedding-3-small-1\")\n",
        "\n",
        "print(\"\\n[INFO] Model Configuration Complete!\")\n",
        "print(\"These LangChain wrappers will be used for:\")\n",
        "print(\"  - LangChain LLM: Faithfulness & Answer Accuracy evaluation\")\n",
        "print(\"  - LangChain Embeddings: Context Recall & Precision evaluation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“ˆ Step 4: Run RAGAS Evaluation\n",
        "\n",
        "Now let's run the RAGAS evaluation using our configured LangChain models and dataset!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Configuring LangChain Models for RAGAS...\n",
            "==================================================\n",
            "[SUCCESS] LangChain LLM configured: gpt-4.1-mini-2025-04-14\n",
            "[SUCCESS] LangChain Embeddings configured: text-embedding-3-small-1\n",
            "\n",
            "[INFO] Model Configuration Complete!\n",
            "These LangChain wrappers will be used for:\n",
            "  - LangChain LLM: Faithfulness & Answer Accuracy evaluation\n",
            "  - LangChain Embeddings: Context Recall & Precision evaluation\n"
          ]
        }
      ],
      "source": [
        "# Configure our EPAM DIAL models for RAGAS\n",
        "print(\"[INFO] Configuring LangChain Models for RAGAS...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create LangChain LLM wrapper (for Faithfulness & Answer Accuracy)\n",
        "# Using original deployments that have access\n",
        "langchain_llm = create_langchain_llm(deployment_name=\"gpt-4.1-mini-2025-04-14\")\n",
        "print(f\"[SUCCESS] LangChain LLM configured: gpt-4.1-mini-2025-04-14\")\n",
        "\n",
        "# Create LangChain Embedding wrapper (for Context Recall & Precision)\n",
        "# Using original deployments that have access\n",
        "langchain_embeddings = create_langchain_embeddings(deployment_name=\"text-embedding-3-small-1\")\n",
        "print(f\"[SUCCESS] LangChain Embeddings configured: text-embedding-3-small-1\")\n",
        "\n",
        "print(\"\\n[INFO] Model Configuration Complete!\")\n",
        "print(\"These LangChain wrappers will be used for:\")\n",
        "print(\"  - LangChain LLM: Faithfulness & Answer Accuracy evaluation\")\n",
        "print(\"  - LangChain Embeddings: Context Recall & Precision evaluation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“ˆStep 5: Run RAGAS Evaluation\n",
        "\n",
        "Now let's run the RAGAS evaluation using our configured models and dataset!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Running RAGAS Evaluation...\n",
            "========================================\n",
            "[INFO] Evaluating metrics:\n",
            "  - ContextRecall\n",
            "  - ContextPrecision\n",
            "  - Faithfulness\n",
            "  - AnswerCorrectness\n",
            "\n",
            "[INFO] Starting evaluation (this may take a few minutes)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d7d27b3b5b945fa8a459d1f74004971",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SUCCESS] Evaluation completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Run RAGAS evaluation\n",
        "print(\"[INFO] Running RAGAS Evaluation...\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Define the metrics we want to evaluate\n",
        "metrics = [\n",
        "    context_recall,\n",
        "    context_precision,\n",
        "    faithfulness,\n",
        "    answer_correctness]\n",
        "\n",
        "print(\"[INFO] Evaluating metrics:\")\n",
        "for metric in metrics:\n",
        "    print(f\"  - {metric.__class__.__name__}\")\n",
        "\n",
        "print(\"\\n[INFO] Starting evaluation (this may take a few minutes)...\")\n",
        "\n",
        "# Run the evaluation\n",
        "try:\n",
        "    result = evaluate(\n",
        "        ragas_dataset,\n",
        "        metrics=metrics,\n",
        "        llm=langchain_llm,\n",
        "        embeddings=langchain_embeddings\n",
        "    )\n",
        "    \n",
        "    print(\"[SUCCESS] Evaluation completed successfully!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"[ERROR] Evaluation failed: {e}\")\n",
        "    print(\"This might be due to API access restrictions or model availability.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š Step 5: Analyze Results\n",
        "\n",
        "Let's analyze the evaluation results and understand what they mean for our RAG system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Analyzing RAGAS Results...\n",
            "========================================\n",
            "[SUCCESS] Overall Scores:\n",
            "{'context_recall': 1.0000, 'context_precision': 1.0000, 'faithfulness': 0.7000, 'answer_correctness': 0.6708}\n",
            "\n",
            "[INFO] Detailed Results:\n",
            "                                user_input  \\\n",
            "0           What is the capital of France?   \n",
            "1  What are the main ingredients in pizza?   \n",
            "2            How does photosynthesis work?   \n",
            "3         What is the population of Tokyo?   \n",
            "4              Who wrote Romeo and Juliet?   \n",
            "\n",
            "                                  retrieved_contexts  \\\n",
            "0  [France is a country located in Western Europe...   \n",
            "1  [Pizza is a popular Italian dish consisting of...   \n",
            "2  [Photosynthesis is the process by which plants...   \n",
            "3  [Tokyo is the capital city of Japan and one of...   \n",
            "4  [Romeo and Juliet is a tragic play written by ...   \n",
            "\n",
            "                                            response  \\\n",
            "0                    The capital of France is Paris.   \n",
            "1  Pizza typically contains dough, tomato sauce, ...   \n",
            "2  Plants use sunlight, water, and carbon dioxide...   \n",
            "3         Tokyo has approximately 14 million people.   \n",
            "4  William Shakespeare wrote Romeo and Juliet, on...   \n",
            "\n",
            "                                           reference  context_recall  \\\n",
            "0                                              Paris             1.0   \n",
            "1          Dough, tomato sauce, cheese, and toppings             1.0   \n",
            "2  Plants convert sunlight, water, and CO2 into g...             1.0   \n",
            "3                    37 million (Greater Tokyo Area)             1.0   \n",
            "4                                William Shakespeare             1.0   \n",
            "\n",
            "   context_precision  faithfulness  answer_correctness  \n",
            "0                1.0           1.0            0.861530  \n",
            "1                1.0           1.0            0.726920  \n",
            "2                1.0           1.0            0.960482  \n",
            "3                1.0           0.0            0.154434  \n",
            "4                1.0           0.5            0.650475  \n",
            "\n",
            "[INFO] Average Scores:\n",
            "  - faithfulness: 0.700\n",
            "\n",
            "[INFO] Result Interpretation:\n",
            "  - Scores range from 0 to 1 (higher is better)\n",
            "  - Context Recall: How complete is the retrieved context?\n",
            "  - Context Precision: How relevant is the retrieved context?\n",
            "  - Faithfulness: How well answers are grounded in context?\n",
            "  - AnswerCorrectness:metric that measures accuracy against ground truth\n"
          ]
        }
      ],
      "source": [
        "# Analyze the results\n",
        "print(\"[INFO] Analyzing RAGAS Results...\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "try:\n",
        "    # Display the results\n",
        "    print(\"[SUCCESS] Overall Scores:\")\n",
        "    print(result)\n",
        "    \n",
        "    # Convert to DataFrame for better analysis\n",
        "    results_df = result.to_pandas()\n",
        "    \n",
        "    print(\"\\n[INFO] Detailed Results:\")\n",
        "    print(results_df)\n",
        "    \n",
        "    # Calculate average scores\n",
        "    print(\"\\n[INFO] Average Scores:\")\n",
        "    for metric in metrics:\n",
        "        metric_name = metric.__class__.__name__.lower()\n",
        "        if metric_name in results_df.columns:\n",
        "            avg_score = results_df[metric_name].mean()\n",
        "            print(f\"  - {metric_name}: {avg_score:.3f}\")\n",
        "    \n",
        "    # Interpret the results\n",
        "    print(\"\\n[INFO] Result Interpretation:\")\n",
        "    print(\"  - Scores range from 0 to 1 (higher is better)\")\n",
        "    print(\"  - Context Recall: How complete is the retrieved context?\")\n",
        "    print(\"  - Context Precision: How relevant is the retrieved context?\")\n",
        "    print(\"  - Faithfulness: How well answers are grounded in context?\")\n",
        "    print(\"  - AnswerCorrectness:metric that measures accuracy against ground truth\")\n",
        "    \n",
        "except NameError:\n",
        "    print(\"[ERROR] No results available. Evaluation may have failed.\")\n",
        "    print(\"Please check your API configuration and try again.\")\n",
        "except Exception as e:\n",
        "    print(f\"[ERROR] Error analyzing results: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ’¾ Step 6: Export Results\n",
        "\n",
        "Let's save our results to a CSV file for further analysis and reporting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Exporting Results...\n",
            "==============================\n",
            "[SUCCESS] Complete evaluation saved to 'ragas_evaluation_results.csv'\n",
            "\n",
            "[INFO] File created:\n",
            "  - ragas_evaluation_results.csv (5 rows)\n",
            "  - Contains: questions, answers, contexts, ground truth, and all RAGAS scores\n"
          ]
        }
      ],
      "source": [
        "# Export results to CSV\n",
        "print(\"[INFO] Exporting Results...\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "try:\n",
        "    results_df.to_csv('ragas_evaluation_results.csv', index=False)\n",
        "    print(\"[SUCCESS] Complete evaluation saved to 'ragas_evaluation_results.csv'\")\n",
        "    \n",
        "    print(f\"\\n[INFO] File created:\")\n",
        "    print(f\"  - ragas_evaluation_results.csv ({len(results_df)} rows)\")\n",
        "    print(f\"  - Contains: questions, answers, contexts, ground truth, and all RAGAS scores\")\n",
        "    \n",
        "except NameError:\n",
        "    print(\"[ERROR] No results to export. Please run the evaluation first.\")\n",
        "except Exception as e:\n",
        "    print(f\"[ERROR] Error exporting results: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ‰ Summary & Next Steps\n",
        "\n",
        "Congratulations! You've successfully set up and run a RAGAS evaluation using EPAM DIAL models.\n",
        "\n",
        "### âœ… What We Accomplished:\n",
        "1. **Connected to EPAM DIAL API** using Azure OpenAI endpoints\n",
        "2. **Created LangChain wrappers** for LLM and embedding models\n",
        "3. **Loaded evaluation dataset** with proper RAGAS format\n",
        "4. **Ran RAGAS metrics** for comprehensive RAG evaluation\n",
        "5. **Analyzed and exported results** for further use\n",
        "\n",
        "### ðŸ”§ RAGAS Metrics Explained:\n",
        "- **Context Recall**: How complete is the retrieved context?\n",
        "- **Context Precision**: How relevant is the retrieved context?\n",
        "- **Faithfulness**: Is the answer grounded in context?\n",
        "- **Answer Relevancy**: How relevant is the answer to the question?\n",
        "\n",
        "### ðŸš€ Next Steps:\n",
        "1. **Real Dataset**: Replace fake data with your actual RAG system data\n",
        "2. **More Metrics**: Add additional RAGAS metrics like `answer_correctness`\n",
        "3. **Batch Evaluation**: Evaluate larger datasets\n",
        "4. **Monitoring**: Set up regular evaluation pipelines\n",
        "5. **Optimization**: Use results to improve your RAG system\n",
        "\n",
        "### ðŸ“š Resources:\n",
        "- [RAGAS Documentation](https://docs.ragas.io/)\n",
        "- [EPAM DIAL Documentation](https://dial.epam.com/)\n",
        "- [LangChain Azure Integration](https://python.langchain.com/docs/integrations/llms/azure_openai)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv_ragas_learn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
