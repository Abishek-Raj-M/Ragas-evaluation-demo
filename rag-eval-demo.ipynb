{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS Evaluation Study: Retrieval-Augmented Generation Assessment\n",
    "\n",
    "This study demonstrates the implementation and analysis of **RAGAS** (Retrieval-Augmented Generation Assessment) for evaluating RAG systems using EPAM DIAL models.\n",
    "\n",
    "## Study Objectives\n",
    "\n",
    "This document covers the following key areas:\n",
    "\n",
    "1. **System Setup** - Configuration of EPAM DIAL API integration\n",
    "2. **Dataset Preparation** - Creation and loading of evaluation datasets\n",
    "3. **Model Configuration** - Setup of LLM and embedding models\n",
    "4. **Evaluation Execution** - Implementation of RAGAS metrics\n",
    "5. **Results Analysis** - Interpretation and export of evaluation results\n",
    "\n",
    "## RAGAS Metrics Framework\n",
    "\n",
    "The following metrics will be evaluated in this study:\n",
    "\n",
    "- **Context Recall** - Measures the completeness of retrieved context relative to ground truth\n",
    "- **Context Precision** - Evaluates the relevance of retrieved context to the query\n",
    "- **Faithfulness** - Assesses how well answers are grounded in the provided context\n",
    "- **Answer Correctness** - Compares answer accuracy against ground truth references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library imports completed successfully.\n",
      "Dependencies loaded:\n",
      "  - pandas: Data manipulation and analysis\n",
      "  - datasets: HuggingFace dataset handling\n",
      "  - ragas: RAG evaluation framework\n",
      "  - utils: Custom EPAM DIAL integration modules\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries and Dependencies\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datasets import Dataset\n",
    "\n",
    "# Import custom utilities for RAGAS evaluation\n",
    "from utils import (\n",
    "    create_ragas_dataset,\n",
    "    create_langchain_llm, \n",
    "    create_langchain_embeddings\n",
    ")\n",
    "\n",
    "# Import RAGAS evaluation framework\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    context_recall,\n",
    "    context_precision, \n",
    "    faithfulness,\n",
    "    answer_correctness\n",
    ")\n",
    "\n",
    "print(\"Library imports completed successfully.\")\n",
    "print(\"Dependencies loaded:\")\n",
    "print(\"  - pandas: Data manipulation and analysis\")\n",
    "print(\"  - datasets: HuggingFace dataset handling\")\n",
    "print(\"  - ragas: RAG evaluation framework\")\n",
    "print(\"  - utils: Custom EPAM DIAL integration modules\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation and Loading\n",
    "\n",
    "This section demonstrates the loading of a diverse evaluation dataset designed to test various RAG system behaviors. The dataset includes carefully crafted examples that demonstrate different combinations of RAGAS metrics.\n",
    "\n",
    "### Dataset Design Principles\n",
    "\n",
    "The evaluation dataset is structured to test the following scenarios:\n",
    "\n",
    "1. **Perfect Performance** - Complete context with accurate answers\n",
    "2. **Context Recall Issues** - Missing important information in retrieved context\n",
    "3. **Context Precision Problems** - Irrelevant information retrieved\n",
    "4. **Faithfulness Violations** - Answers not grounded in provided context\n",
    "5. **Answer Correctness Errors** - Incorrect answers despite good context\n",
    "6. **Mixed Scenarios** - Various combinations of the above issues\n",
    "7. **Partial Context Recall** - Some relevant information missing\n",
    "8. **High Precision, Low Recall** - Highly relevant but incomplete context\n",
    "\n",
    "This design allows for comprehensive evaluation of RAG system performance across different failure modes.\n",
    "\n",
    "This section load our **diverse dataset** that demonstrates different combinations of RAGAS metrics. This dataset includes 8 carefully crafted examples that will show:\n",
    "\n",
    "###  Dataset Scenarios:\n",
    "1. **PERFECT SCORES** - All metrics should be high\n",
    "2. **LOW CONTEXT RECALL** - Missing important information  \n",
    "3. **LOW CONTEXT PRECISION** - Irrelevant context retrieved\n",
    "4. **LOW FAITHFULNESS** - Answer not grounded in context (hallucination)\n",
    "5. **LOW ANSWER CORRECTNESS** - Wrong answer despite good context\n",
    "6. **MIXED SCENARIO** - Good context but partial answer\n",
    "7. **PARTIAL CONTEXT RECALL** - Some relevant info missing\n",
    "8. **HIGH PRECISION, LOW RECALL** - Very relevant but incomplete\n",
    "\n",
    "This will help us understand how each RAGAS metric behaves in different scenarios!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information: Loading Diverse Evaluation Dataset...\n",
      "==================================================\n",
      "Dataset shape: (8, 5)\n",
      "Columns: ['question', 'answer', 'context', 'ground_truth', 'retrieved_contexts']\n",
      "\n",
      "First few rows:\n",
      "                                  question  \\\n",
      "0           What is the capital of France?   \n",
      "1  What are the main ingredients in pizza?   \n",
      "2            How does photosynthesis work?   \n",
      "3         What is the population of Tokyo?   \n",
      "4              Who wrote Romeo and Juliet?   \n",
      "\n",
      "                                              answer  \\\n",
      "0                    The capital of France is Paris.   \n",
      "1  Pizza contains dough, tomato sauce, cheese, an...   \n",
      "2  Plants use sunlight, water, and carbon dioxide...   \n",
      "3  Tokyo has approximately 50 million people and ...   \n",
      "4    Charles Dickens wrote Romeo and Juliet in 1850.   \n",
      "\n",
      "                                             context  \\\n",
      "0  France is a country located in Western Europe....   \n",
      "1                   Pizza is a popular Italian dish.   \n",
      "2  Cooking is a great hobby. Many people enjoy pr...   \n",
      "3  Tokyo is the capital city of Japan. It is know...   \n",
      "4  Romeo and Juliet is a tragic play written by W...   \n",
      "\n",
      "                                        ground_truth  \\\n",
      "0                                              Paris   \n",
      "1  Dough, tomato sauce, cheese, and various toppings   \n",
      "2  Plants convert sunlight, water, and CO2 into g...   \n",
      "3                    37 million (Greater Tokyo Area)   \n",
      "4                                William Shakespeare   \n",
      "\n",
      "                                  retrieved_contexts  \n",
      "0  [France is a country located in Western Europe...  \n",
      "1                 [Pizza is a popular Italian dish.]  \n",
      "2  [Cooking is a great hobby. Many people enjoy p...  \n",
      "3  [Tokyo is the capital city of Japan. It is kno...  \n",
      "4  [Romeo and Juliet is a tragic play written by ...  \n",
      "\n",
      "Information: Dataset Summary:\n",
      "  - Total examples: 8\n",
      "  - Questions: 8 unique\n",
      "  - Average answer length: 68.1 characters\n",
      "  - Average context length: 167.4 characters\n",
      "\n",
      "Information: Dataset Scenarios:\n",
      "  1. PERFECT SCORES - All metrics should be high\n",
      "  2. LOW CONTEXT RECALL - Missing important information\n",
      "  3. LOW CONTEXT PRECISION - Irrelevant context retrieved\n",
      "  4. LOW FAITHFULNESS - Answer not grounded in context (hallucination)\n",
      "  5. LOW ANSWER CORRECTNESS - Wrong answer despite good context\n",
      "  6. MIXED SCENARIO - Good context but partial answer\n",
      "  7. PARTIAL CONTEXT RECALL - Some relevant info missing\n",
      "  8. HIGH PRECISION, LOW RECALL - Very relevant but incomplete\n",
      "\n",
      "Status: Success Diverse dataset ready for RAGAS evaluation!\n"
     ]
    }
   ],
   "source": [
    "# Load our diverse dataset\n",
    "print(\"Information: Loading Diverse Evaluation Dataset...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create the dataset directly as RAGAS-compatible format\n",
    "ragas_dataset = create_ragas_dataset()\n",
    "\n",
    "print(f\"Dataset shape: {ragas_dataset.shape}\")\n",
    "print(f\"Columns: {ragas_dataset.column_names}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(ragas_dataset.to_pandas().head())\n",
    "\n",
    "print(\"\\nInformation: Dataset Summary:\")\n",
    "print(f\"  - Total examples: {len(ragas_dataset)}\")\n",
    "print(f\"  - Questions: {len(set(ragas_dataset['question']))} unique\")\n",
    "print(f\"  - Average answer length: {sum(len(a) for a in ragas_dataset['answer']) / len(ragas_dataset):.1f} characters\")\n",
    "print(f\"  - Average context length: {sum(len(c) for c in ragas_dataset['context']) / len(ragas_dataset):.1f} characters\")\n",
    "\n",
    "print(f\"\\nInformation: Dataset Scenarios:\")\n",
    "scenarios = [\n",
    "    \"1. PERFECT SCORES - All metrics should be high\",\n",
    "    \"2. LOW CONTEXT RECALL - Missing important information\", \n",
    "    \"3. LOW CONTEXT PRECISION - Irrelevant context retrieved\",\n",
    "    \"4. LOW FAITHFULNESS - Answer not grounded in context (hallucination)\",\n",
    "    \"5. LOW ANSWER CORRECTNESS - Wrong answer despite good context\",\n",
    "    \"6. MIXED SCENARIO - Good context but partial answer\",\n",
    "    \"7. PARTIAL CONTEXT RECALL - Some relevant info missing\",\n",
    "    \"8. HIGH PRECISION, LOW RECALL - Very relevant but incomplete\"\n",
    "]\n",
    "\n",
    "for scenario in scenarios:\n",
    "    print(f\"  {scenario}\")\n",
    "\n",
    "print(f\"\\nStatus: Success Diverse dataset ready for RAGAS evaluation!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Configure LangChain Models for RAGAS\n",
    "\n",
    "This section create our LangChain wrappers for the EPAM DIAL models that RAGAS will use for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information: Configuring LangChain Models for RAGAS...\n",
      "==================================================\n",
      "Status: Success LangChain LLM configured: gpt-4.1-mini-2025-04-14\n",
      "Status: Success LangChain Embeddings configured: text-embedding-3-small-1\n",
      "\n",
      "Information: Model Configuration Complete!\n",
      "These LangChain wrappers will be used for:\n",
      "  - LangChain LLM: Faithfulness & Answer Accuracy evaluation\n",
      "  - LangChain Embeddings: Context Recall & Precision evaluation\n"
     ]
    }
   ],
   "source": [
    "# Configure our EPAM DIAL models for RAGAS\n",
    "print(\"Information: Configuring LangChain Models for RAGAS...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create LangChain LLM wrapper (for Faithfulness & Answer Accuracy)\n",
    "# Using original deployments that have access\n",
    "langchain_llm = create_langchain_llm(deployment_name=\"gpt-4.1-mini-2025-04-14\")\n",
    "print(f\"Status: Success LangChain LLM configured: gpt-4.1-mini-2025-04-14\")\n",
    "\n",
    "# Create LangChain Embedding wrapper (for Context Recall & Precision)\n",
    "# Using original deployments that have access\n",
    "langchain_embeddings = create_langchain_embeddings(deployment_name=\"text-embedding-3-small-1\")\n",
    "print(f\"Status: Success LangChain Embeddings configured: text-embedding-3-small-1\")\n",
    "\n",
    "print(\"\\nInformation: Model Configuration Complete!\")\n",
    "print(\"These LangChain wrappers will be used for:\")\n",
    "print(\"  - LangChain LLM: Faithfulness & Answer Accuracy evaluation\")\n",
    "print(\"  - LangChain Embeddings: Context Recall & Precision evaluation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Run RAGAS Evaluation\n",
    "\n",
    "This section run the RAGAS evaluation using our configured models and dataset!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information: Running RAGAS Evaluation...\n",
      "========================================\n",
      "Information: Evaluating metrics:\n",
      "  - ContextRecall\n",
      "  - ContextPrecision\n",
      "  - Faithfulness\n",
      "  - AnswerCorrectness\n",
      "\n",
      "Information: Starting evaluation (this may take a few minutes)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8690d4364b409990bf070681dfe16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Success Evaluation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Run RAGAS evaluation\n",
    "print(\"Information: Running RAGAS Evaluation...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Define the metrics we want to evaluate\n",
    "metrics = [\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    faithfulness,\n",
    "    answer_correctness]\n",
    "\n",
    "print(\"Information: Evaluating metrics:\")\n",
    "for metric in metrics:\n",
    "    print(f\"  - {metric.__class__.__name__}\")\n",
    "\n",
    "print(\"\\nInformation: Starting evaluation (this may take a few minutes)...\")\n",
    "\n",
    "# Run the evaluation\n",
    "try:\n",
    "    result = evaluate(\n",
    "        ragas_dataset,\n",
    "        metrics=metrics,\n",
    "        llm=langchain_llm,\n",
    "        embeddings=langchain_embeddings\n",
    "    )\n",
    "    \n",
    "    print(\"Status: Success Evaluation completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: Evaluation failed: {e}\")\n",
    "    print(\"This might be due to API access restrictions or model availability.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Analyze Results & Understand Metric Variations\n",
    "\n",
    "This section analyze the evaluation results and understand what they mean for our RAG system. With our diverse dataset, we should see different score patterns that demonstrate how each RAGAS metric works.\n",
    "\n",
    "###  Expected Results by Scenario:\n",
    "- **Example 1 (France)**: High scores across all metrics (perfect scenario)\n",
    "- **Example 2 (Pizza)**: Low Context Recall (missing ingredient details)\n",
    "- **Example 3 (Photosynthesis)**: Low Context Precision (irrelevant cooking info)\n",
    "- **Example 4 (Tokyo)**: Low Faithfulness (hallucinated population data)\n",
    "- **Example 5 (Romeo & Juliet)**: Low Answer Correctness (wrong author)\n",
    "- **Example 6 (Exercise)**: Mixed scores (good context, partial answer)\n",
    "- **Example 7 (Machine Learning)**: Low Context Recall (missing details)\n",
    "- **Example 8 (Speed of Light)**: High Precision, Low Recall (precise but minimal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information: Analyzing RAGAS Results...\n",
      "==================================================\n",
      "Status: Success Overall Scores:\n",
      "{'context_recall': 0.5000, 'context_precision': 0.6250, 'faithfulness': 0.4375, 'answer_correctness': 0.6004}\n",
      "\n",
      "Information: Detailed Results by Example:\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 1: What is the capital of France?\n",
      "Scenario: 1. PERFECT SCORES - All metrics should be high\n",
      "Context Recall: 1.000\n",
      "Context Precision: 1.000\n",
      "Faithfulness: 1.000\n",
      "Answer Correctness: 0.862\n",
      "\n",
      "Example 2: What are the main ingredients in pizza?\n",
      "Scenario: 2. LOW CONTEXT RECALL - Missing important information\n",
      "Context Recall: 0.000\n",
      "Context Precision: 0.000\n",
      "Faithfulness: 0.000\n",
      "Answer Correctness: 0.941\n",
      "\n",
      "Example 3: How does photosynthesis work?\n",
      "Scenario: 3. LOW CONTEXT PRECISION - Irrelevant context retrieved\n",
      "Context Recall: 0.000\n",
      "Context Precision: 0.000\n",
      "Faithfulness: 0.000\n",
      "Answer Correctness: 0.960\n",
      "\n",
      "Example 4: What is the population of Tokyo?\n",
      "Scenario: 4. LOW FAITHFULNESS - Answer not grounded in context (hallucination)\n",
      "Context Recall: 0.000\n",
      "Context Precision: 0.000\n",
      "Faithfulness: 0.000\n",
      "Answer Correctness: 0.135\n",
      "\n",
      "Example 5: Who wrote Romeo and Juliet?\n",
      "Scenario: 5. LOW ANSWER CORRECTNESS - Wrong answer despite good context\n",
      "Context Recall: 1.000\n",
      "Context Precision: 1.000\n",
      "Faithfulness: 0.000\n",
      "Answer Correctness: 0.101\n",
      "\n",
      "Example 6: What are the benefits of exercise?\n",
      "Scenario: 6. MIXED SCENARIO - Good context but partial answer\n",
      "Context Recall: 1.000\n",
      "Context Precision: 1.000\n",
      "Faithfulness: 1.000\n",
      "Answer Correctness: 0.454\n",
      "\n",
      "Example 7: What is machine learning?\n",
      "Scenario: 7. PARTIAL CONTEXT RECALL - Some relevant info missing\n",
      "Context Recall: 0.000\n",
      "Context Precision: 1.000\n",
      "Faithfulness: 0.500\n",
      "Answer Correctness: 0.448\n",
      "\n",
      "Example 8: What is the speed of light?\n",
      "Scenario: 8. HIGH PRECISION, LOW RECALL - Very relevant but incomplete\n",
      "Context Recall: 1.000\n",
      "Context Precision: 1.000\n",
      "Faithfulness: 1.000\n",
      "Answer Correctness: 0.902\n",
      "\n",
      "Information: Average Scores Across All Examples:\n",
      "--------------------------------------------------\n",
      "  - faithfulness: 0.438\n",
      "\n",
      "Information: Metric Interpretation Guide:\n",
      "--------------------------------------------------\n",
      "  - Scores range from 0 to 1 (higher is better)\n",
      "  - Context Recall: How complete is the retrieved context?\n",
      "  - Context Precision: How relevant is the retrieved context?\n",
      "  - Faithfulness: How well answers are grounded in context?\n",
      "  - Answer Correctness: How accurate is the answer vs ground truth?\n",
      "\n",
      "Information: Analysis Summary\n",
      "------------------------------\n",
      "  - Look for patterns in the scores across different scenarios\n",
      "  - Notice how different types of problems affect different metrics\n",
      "  - Use these insights to improve your RAG system!\n"
     ]
    }
   ],
   "source": [
    "# Analyze the results with detailed explanations\n",
    "print(\"Information: Analyzing RAGAS Results...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Display the results\n",
    "    print(\"Status: Success Overall Scores:\")\n",
    "    print(result)\n",
    "    \n",
    "    # Convert to DataFrame for better analysis\n",
    "    results_df = result.to_pandas()\n",
    "    \n",
    "    print(\"\\nInformation: Detailed Results by Example:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Add scenario descriptions to results\n",
    "    scenarios = [\n",
    "        \"1. PERFECT SCORES - All metrics should be high\",\n",
    "        \"2. LOW CONTEXT RECALL - Missing important information\", \n",
    "        \"3. LOW CONTEXT PRECISION - Irrelevant context retrieved\",\n",
    "        \"4. LOW FAITHFULNESS - Answer not grounded in context (hallucination)\",\n",
    "        \"5. LOW ANSWER CORRECTNESS - Wrong answer despite good context\",\n",
    "        \"6. MIXED SCENARIO - Good context but partial answer\",\n",
    "        \"7. PARTIAL CONTEXT RECALL - Some relevant info missing\",\n",
    "        \"8. HIGH PRECISION, LOW RECALL - Very relevant but incomplete\"\n",
    "    ]\n",
    "    \n",
    "    for i, (idx, row) in enumerate(results_df.iterrows()):\n",
    "        print(f\"\\nExample {i+1}: {row['user_input']}\")\n",
    "        print(f\"Scenario: {scenarios[i]}\")\n",
    "        print(f\"Context Recall: {row['context_recall']:.3f}\")\n",
    "        print(f\"Context Precision: {row['context_precision']:.3f}\")\n",
    "        print(f\"Faithfulness: {row['faithfulness']:.3f}\")\n",
    "        print(f\"Answer Correctness: {row['answer_correctness']:.3f}\")\n",
    "    \n",
    "    # Calculate average scores\n",
    "    print(\"\\nInformation: Average Scores Across All Examples:\")\n",
    "    print(\"-\" * 50)\n",
    "    for metric in metrics:\n",
    "        metric_name = metric.__class__.__name__.lower()\n",
    "        if metric_name in results_df.columns:\n",
    "            avg_score = results_df[metric_name].mean()\n",
    "            print(f\"  - {metric_name}: {avg_score:.3f}\")\n",
    "    \n",
    "    # Interpret the results\n",
    "    print(\"\\nInformation: Metric Interpretation Guide:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"  - Scores range from 0 to 1 (higher is better)\")\n",
    "    print(\"  - Context Recall: How complete is the retrieved context?\")\n",
    "    print(\"  - Context Precision: How relevant is the retrieved context?\")\n",
    "    print(\"  - Faithfulness: How well answers are grounded in context?\")\n",
    "    print(\"  - Answer Correctness: How accurate is the answer vs ground truth?\")\n",
    "    \n",
    "    print(\"\\nInformation: Analysis Summary\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"  - Look for patterns in the scores across different scenarios\")\n",
    "    print(\"  - Notice how different types of problems affect different metrics\")\n",
    "    print(\"  - Use these insights to improve your RAG system!\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"Error: No results available. Evaluation may have failed.\")\n",
    "    print(\"Please check your API configuration and try again.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: Error analyzing results: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Export Results\n",
    "\n",
    "This section save our results to a CSV file for further analysis and reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information: Exporting Results...\n",
      "==============================\n",
      "Status: Success Complete evaluation saved to 'ragas_evaluation_results.csv'\n",
      "\n",
      "Information: File created:\n",
      "  - ragas_evaluation_results.csv (8 rows)\n",
      "  - Contains: questions, answers, contexts, ground truth, and all RAGAS scores\n"
     ]
    }
   ],
   "source": [
    "# Export results to CSV\n",
    "print(\"Information: Exporting Results...\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "try:\n",
    "    results_df.to_csv('ragas_evaluation_results.csv', index=False)\n",
    "    print(\"Status: Success Complete evaluation saved to 'ragas_evaluation_results.csv'\")\n",
    "    \n",
    "    print(f\"\\nInformation: File created:\")\n",
    "    print(f\"  - ragas_evaluation_results.csv ({len(results_df)} rows)\")\n",
    "    print(f\"  - Contains: questions, answers, contexts, ground truth, and all RAGAS scores\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"Error: No results to export. Please run the evaluation first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: Error exporting results: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Summary & Key Learnings\n",
    "\n",
    "Congratulations! You've successfully set up and run a RAGAS evaluation using EPAM DIAL models with a **diverse dataset** that demonstrates different metric combinations.\n",
    "\n",
    "###  Study Results\n",
    "1. **Connected to EPAM DIAL API** using Azure OpenAI endpoints\n",
    "2. **Created LangChain wrappers** for LLM and embedding models\n",
    "3. **Loaded diverse evaluation dataset** with 8 carefully crafted scenarios\n",
    "4. **Ran RAGAS metrics** for comprehensive RAG evaluation\n",
    "5. **Analyzed metric variations** across different problem types\n",
    "6. **Exported results** for further analysis\n",
    "\n",
    "###  RAGAS Metrics Demonstrated:\n",
    "- **Context Recall**: How complete is the retrieved context?\n",
    "  - *Low scores*: Missing important information (Examples 2, 7)\n",
    "  - *High scores*: Complete relevant information (Examples 1, 8)\n",
    "  \n",
    "- **Context Precision**: How relevant is the retrieved context?\n",
    "  - *Low scores*: Irrelevant information retrieved (Example 3)\n",
    "  - *High scores*: Highly relevant context (Examples 1, 8)\n",
    "  \n",
    "- **Faithfulness**: Is the answer grounded in context?\n",
    "  - *Low scores*: Hallucinated information (Example 4)\n",
    "  - *High scores*: Well-grounded answers (Examples 1, 6)\n",
    "  \n",
    "- **Answer Correctness**: How accurate is the answer vs ground truth?\n",
    "  - *Low scores*: Wrong answers despite good context (Example 5)\n",
    "  - *High scores*: Accurate answers (Examples 1, 8)\n",
    "\n",
    "###  Recommendations\n",
    "1. **Real Dataset**: Replace diverse examples with your actual RAG system data\n",
    "2. **More Metrics**: Add additional RAGAS metrics like `answer_relevancy`\n",
    "3. **Batch Evaluation**: Evaluate larger datasets with real retrieval systems\n",
    "4. **Monitoring**: Set up regular evaluation pipelines\n",
    "5. **Optimization**: Use results to improve your RAG system's retrieval and generation\n",
    "\n",
    "###  Resources:\n",
    "- [RAGAS Documentation](https://docs.ragas.io/)\n",
    "- [EPAM DIAL Documentation](https://dial.epam.com/)\n",
    "- [LangChain Azure Integration](https://python.langchain.com/docs/integrations/llms/azure_openai)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_ragas_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
